{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a163df6-1344-43d2-afd2-980d581ed2b6",
   "metadata": {},
   "source": [
    "## The purpose of this project is to implement a BERT-based model for sentiment binary classification on the Twitter US Airlines Sentiment dataset. ##\n",
    "The key columns are:  \n",
    "* text: The content of the tweets\n",
    "* airline_sentiment: The sentiment labels for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c79a77c-aba3-4f33-8e9c-8791a221e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries \n",
    "import torch # for deep learning\n",
    "import numpy as np # for numerical operations\n",
    "import torch.nn as nn  # Neural network \n",
    "import torch.optim as optim  # Optimizers for training\n",
    "from transformers import BertTokenizer, BertModel  # Pre-trained BERT tokenizer and model\n",
    "from sklearn.model_selection import train_test_split  # Split data into training, validation, and test sets\n",
    "import time  # To measure training time\n",
    "from torch.utils.data import DataLoader, Dataset  # Classes for handling datasets and batching\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e816e29-b405-4405-a088-2ba23bc78747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "def load_dataset():\n",
    "    df = pd.read_csv(\"Tweets.csv\")\n",
    "    texts = df['text'].tolist()\n",
    "    sentiment_map = {'positive': 2, 'neutral': 0, 'negative': 1}\n",
    "    labels = dataset['airline_sentiment'].map(sentiment_map).tolist()\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b11b7162-e80b-4119-b435-e4585a28d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.916 | Train Acc: 59.91% | Valid Loss: 0.835 | Valid Acc: 63.61% | Time: 428.92s\n",
      "Epoch 2 | Train Loss: 0.845 | Train Acc: 61.61% | Valid Loss: 0.774 | Valid Acc: 63.80% | Time: 422.78s\n",
      "Epoch 3 | Train Loss: 0.780 | Train Acc: 64.43% | Valid Loss: 0.713 | Valid Acc: 68.61% | Time: 419.73s\n",
      "Epoch 4 | Train Loss: 0.722 | Train Acc: 68.86% | Valid Loss: 0.649 | Valid Acc: 73.75% | Time: 421.09s\n",
      "Epoch 5 | Train Loss: 0.663 | Train Acc: 72.77% | Valid Loss: 0.595 | Valid Acc: 75.16% | Time: 420.42s\n",
      "Test Loss: 0.599 | Test Acc: 75.53%\n",
      "('Positive', [0.14642806351184845, 0.05673369765281677, 0.7968382835388184])\n",
      "('Neutral', [0.12194711714982986, 0.8108943700790405, 0.06715847551822662])\n",
      "('Neutral', [0.26911893486976624, 0.6571492552757263, 0.07373186200857162])\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility \n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "# Define dataset class for text classification\n",
    "class AirlineSentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "        'input_ids': tokens['input_ids'].flatten(), \n",
    "        'attention_mask': tokens['attention_mask'].flatten(), \n",
    "        'label': torch.tensor(label, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# Extract key columns \n",
    "texts, labels = load_dataset()\n",
    "\n",
    "# Split the dataset into test and training \n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.4, random_state=SEED)\n",
    "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=0.5, random_state=SEED)\n",
    "\n",
    "# Create dataset objects for each split \n",
    "train_dataset = AirlineSentimentDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "valid_dataset = AirlineSentimentDataset(valid_texts, valid_labels, tokenizer, max_length)\n",
    "test_dataset = AirlineSentimentDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "\n",
    "# Set batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create DataLoader to batch the data \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define the BERT-GRU model\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert  # Pre-trained BERT model\n",
    "        self.rnn = nn.GRU(bert.config.hidden_size, hidden_dim, batch_first=True, bidirectional=True)  # GRU layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Fully connected layer for binary classification\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass input through BERT and extract embeddings\n",
    "        with torch.no_grad():  # Freeze BERT weights\n",
    "            embedded = self.bert(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        # Pass embeddings through GRU\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        # Concatenate the last forward and backward GRU hidden states\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        return self.fc(hidden)  # Pass through the fully connected layer\n",
    "\n",
    "# Initialize the BERT-GRU model\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BERTGRUSentiment(bert, hidden_dim=128, output_dim=3, dropout=0.3).to(torch.device(\"cpu\"))\n",
    "\n",
    "# Freeze BERT parameters to avoid updating them during training\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)  # Adam optimizer\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(preds, y):\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    correct = (predicted == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        acc = calculate_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['label']\n",
    "            \n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = calculate_accuracy(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 4\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
    "    end_time = time.time()\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'bert_gru_model.pt')\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('bert_gru_model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Prediction function for multi-class classification\n",
    "def predict_sentiment(model, tokenizer, text):\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_ids, attention_mask)\n",
    "        _, predicted_class = torch.max(prediction, 1)\n",
    "        probabilities = torch.softmax(prediction, dim=1)\n",
    "    \n",
    "    sentiment_classes = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    return sentiment_classes[predicted_class.item()], probabilities[0].tolist()\n",
    "\n",
    "# Example predictions\n",
    "print(predict_sentiment(model, tokenizer, \"VirginAmerica The flight was amazing, thank you!\"))\n",
    "print(predict_sentiment(model, tokenizer, \"AmericanAir worst customer service ever. Never flying with you again.\"))\n",
    "print(predict_sentiment(model, tokenizer, \"SouthwestAir Flight delayed but staff was helpful.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac187778-3593-4a9f-bcdf-4c001ff4c66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca4cc2-0ab4-4374-94d0-5f5fc5151d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MachineLearning)",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
