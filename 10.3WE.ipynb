{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a339e22-fb37-4564-9684-6a828cfa07e4",
   "metadata": {},
   "source": [
    "## Finding Similar Words Using GloVe Embeddings ##\n",
    "The purpose of this assignment is to find the most similar words using the GloVe model. While the CBOW model typically predicts a target word based on its context words, this project leverages pre-trained word embeddings from the GloVe model to enhance similarity comparisons.  \n",
    "Project Overview\n",
    "This implementation:   \n",
    "* Utilizes pre-trained GloVe embeddings that capture rich semantic relationships  \n",
    "* Applies vector similarity metrics (cosine similarity) to identify words with similar meanings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc24d39b-a325-473b-b044-c0831d2b678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe as GloVeVectors\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cba497-857c-47af-8596-6557d42c22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings \n",
    "# Load pre-trained GloVe embeddings (50-dimensional)\n",
    "glove = GloVeVectors(name=\"6B\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b1e842-927f-4f97-8da0-3c09a89fa7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get word vectors \n",
    "def words_vectors(word):\n",
    "    # ensure word index exists \n",
    "    if word in glove.stoi:\n",
    "        # Get an index of the word in the vocabulary\n",
    "        index = glove.stoi[word]\n",
    "        return glove.vectors[index].numpy()\n",
    "    else:\n",
    "        print(word, \"is not in the vocabulary\")\n",
    "        # return zero vector with same dimensions \n",
    "        return np.zeros(glove.vectors.shape[1])\n",
    "\n",
    "# function to calculate cosine similaries \n",
    "def cosine_similarity(word1, word2):\n",
    "    vec1 = words_vectors(word1)\n",
    "    vec2 = words_vectors(word2)\n",
    "    # Convert numpy arrays to Pytorch tensors\n",
    "    vec1_tensor = torch.tensor(vec1)\n",
    "    vec2_tensor = torch.tensor(vec2)\n",
    "    # unsequeeze \n",
    "    vec1_tensor = vec1_tensor.unsqueeze(0)\n",
    "    vec2_tensor = vec2_tensor.unsqueeze(0)\n",
    "    # using building built-in function \n",
    "    cos_sim = torch.cosine_similarity(vec1_tensor,vec2_tensor)\n",
    "    return cos_sim.item()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ccc346b-1d26-40f5-9da1-61f13266fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity:  dog  and  whale is:  0.6506090760231018\n",
      "Cosine similarity:  dog  and  before is:  0.419243723154068\n",
      "Cosine similarity:  dog  and  however is:  0.36520859599113464\n",
      "Cosine similarity:  dog  and  fabricate is:  -0.02227747067809105\n",
      "Cosine similarity:  whale  and  dog is:  0.6506090760231018\n",
      "Cosine similarity:  whale  and  before is:  0.20935004949569702\n",
      "Cosine similarity:  whale  and  however is:  0.17823582887649536\n",
      "Cosine similarity:  whale  and  fabricate is:  -0.09092274308204651\n",
      "Cosine similarity:  before  and  dog is:  0.419243723154068\n",
      "Cosine similarity:  before  and  whale is:  0.20935004949569702\n",
      "Cosine similarity:  before  and  however is:  0.78252112865448\n",
      "Cosine similarity:  before  and  fabricate is:  -0.1622106283903122\n",
      "Cosine similarity:  however  and  dog is:  0.36520859599113464\n",
      "Cosine similarity:  however  and  whale is:  0.17823582887649536\n",
      "Cosine similarity:  however  and  before is:  0.78252112865448\n",
      "Cosine similarity:  however  and  fabricate is:  -0.037840478122234344\n",
      "Cosine similarity:  fabricate  and  dog is:  -0.02227747067809105\n",
      "Cosine similarity:  fabricate  and  whale is:  -0.09092274308204651\n",
      "Cosine similarity:  fabricate  and  before is:  -0.1622106283903122\n",
      "Cosine similarity:  fabricate  and  however is:  -0.037840478122234344\n"
     ]
    }
   ],
   "source": [
    "# words for testing\n",
    "words = ['dog', 'whale', 'before', 'however', 'fabricate']\n",
    "# print each pair with similaries \n",
    "for i, word1 in enumerate(words):\n",
    "    for j, word2 in enumerate(words):\n",
    "        if i != j:\n",
    "            similarity = cosine_similarity(word1, word2)\n",
    "            print(\"Cosine similarity: \", word1, \" and \", word2, \"is: \", similarity)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10ed98-e253-4d9c-bad0-592554cc7dbf",
   "metadata": {},
   "source": [
    "## The highest cosine similarity in this word set is between \"before\" and \"however\" at 0.78252, or approximately 78.3% ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374a0dc-4b7f-4db3-a44b-9791ac76e63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f9c5b-7983-4039-bfbb-2df56e7c31d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MachineLearning)",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
